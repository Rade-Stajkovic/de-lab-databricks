{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6ddbff-d326-4741-85eb-c224ce5ab65e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ---------- 1. Get pr_id parameter ----------\n",
    "dbutils.widgets.text(\"pr_id\", \"local_dev\")  # default value for manual runs\n",
    "pr_id = dbutils.widgets.get(\"pr_id\")\n",
    "\n",
    "print(f\"Running pipeline for pr_id = '{pr_id}'\")\n",
    "\n",
    "# ---------- 2. Build raw & clean database names ----------\n",
    "if pr_id == \"prod\":\n",
    "    raw_db_name = \"raw\"\n",
    "    clean_db_name = \"clean\"\n",
    "else:\n",
    "    raw_db_name = f\"{pr_id}_raw\"\n",
    "    clean_db_name = f\"{pr_id}_clean\"\n",
    "\n",
    "print(f\"Using raw DB   = {raw_db_name}\")\n",
    "print(f\"Using clean DB = {clean_db_name}\")\n",
    "\n",
    "# Make sure the databases (schemas) exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {raw_db_name}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {clean_db_name}\")\n",
    "\n",
    "# ---------- 3. Read from raw table ----------\n",
    "raw_table = f\"{raw_db_name}.orders_raw\"\n",
    "\n",
    "print(f\"Reading from raw table: {raw_table}\")\n",
    "\n",
    "orders_df = spark.table(raw_table)\n",
    "\n",
    "print(\"Raw schema:\")\n",
    "orders_df.printSchema()\n",
    "print(\"Sample raw data:\")\n",
    "display(orders_df.limit(10))\n",
    "\n",
    "# ---------- 4. Transformations (example logic) ----------\n",
    "orders_enriched_df = (\n",
    "    orders_df\n",
    "    .withColumn(\"order_year\", F.year(\"created_at\"))\n",
    "    .withColumn(\"amount_eur\", F.col(\"amount\") * F.lit(0.93))  # fake FX rate\n",
    "    .filter(F.col(\"amount\") > 0)\n",
    ")\n",
    "\n",
    "print(\"Enriched schema:\")\n",
    "orders_enriched_df.printSchema()\n",
    "print(\"Sample enriched data:\")\n",
    "display(orders_enriched_df.limit(10))\n",
    "\n",
    "# ---------- 5. Write to clean table ----------\n",
    "clean_table = f\"{clean_db_name}.orders_enriched\"\n",
    "\n",
    "print(f\"Writing to clean table: {clean_table}\")\n",
    "\n",
    "(\n",
    "    orders_enriched_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(clean_table)\n",
    ")\n",
    "\n",
    "print(\"Pipeline finished successfully âœ…\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "my_pipeline_notebook",
   "widgets": {
    "pr_id": {
     "currentValue": "pr_test",
     "nuid": "f98e5555-38ce-4ca0-a280-31c25f225f78",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "local_dev",
      "label": null,
      "name": "pr_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "local_dev",
      "label": null,
      "name": "pr_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
