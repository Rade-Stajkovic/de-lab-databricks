{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66534bdb-f2d5-4438-8bed-c7c8ec4959de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import json\n",
    "\n",
    "# ---------- 1. Get pr_id parameter ----------\n",
    "dbutils.widgets.text(\"pr_id\", \"local_dev\")  # default for manual testing\n",
    "pr_id = dbutils.widgets.get(\"pr_id\")\n",
    "\n",
    "assert pr_id, \"pr_id is required\"\n",
    "\n",
    "print(f\"Initializing test data for pr_id = '{pr_id}'\")\n",
    "\n",
    "# ---------- 2. Build raw DB name ----------\n",
    "if pr_id == \"prod\":\n",
    "    raw_db_name = \"raw\"\n",
    "else:\n",
    "    raw_db_name = f\"{pr_id}_raw\"\n",
    "\n",
    "print(f\"Using raw DB = {raw_db_name}\")\n",
    "\n",
    "# Make sure the database (schema) exists\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {raw_db_name}\")\n",
    "\n",
    "# ---------- 3. Load test dataset from JSON file in the repo ----------\n",
    "\n",
    "# e.g. /Workspace/Repos/you/de-lab-databricks/notebooks/tests/init_test_notebook\n",
    "input_path = \"/Workspace/Repos/radomir@elfak.rs/de-lab-databricks/tests/input/orders_input.json\"\n",
    "print(f\"Reading test data from: {input_path}\")\n",
    "\n",
    "with open(input_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Raw Python data loaded from JSON:\")\n",
    "print(data)\n",
    "\n",
    "# 2) Define schema\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.StringType(), False),\n",
    "    T.StructField(\"customer_id\", T.StringType(), False),\n",
    "    T.StructField(\"amount\", T.DoubleType(), False),\n",
    "    T.StructField(\"created_at\", T.StringType(), False),  # string first\n",
    "])\n",
    "\n",
    "# 3) Create DataFrame and convert created_at to timestamp\n",
    "df_raw = (\n",
    "    spark.createDataFrame(data, schema)\n",
    "         .withColumn(\"created_at\", F.to_timestamp(\"created_at\"))\n",
    ")\n",
    "\n",
    "print(\"Test raw DataFrame:\")\n",
    "display(df_raw)\n",
    "\n",
    "# ---------- 4. Write to raw table ----------\n",
    "raw_table = f\"{raw_db_name}.orders_raw\"\n",
    "print(f\"Writing test data to raw table: {raw_table}\")\n",
    "\n",
    "(\n",
    "    df_raw\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(raw_table)\n",
    ")\n",
    "\n",
    "print(\"Raw test data initialized âœ…\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "init_test_notebook",
   "widgets": {
    "pr_id": {
     "currentValue": "pr_test",
     "nuid": "53c6cbba-9c48-4531-94cb-55569fc9fa00",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "local_dev",
      "label": null,
      "name": "pr_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "local_dev",
      "label": null,
      "name": "pr_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
