{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66534bdb-f2d5-4438-8bed-c7c8ec4959de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# ---------- 1. Get pr_id parameter ----------\n",
    "dbutils.widgets.text(\"pr_id\", \"local_dev\")  # default for manual testing\n",
    "pr_id = dbutils.widgets.get(\"pr_id\")\n",
    "\n",
    "assert pr_id, \"pr_id is required\"\n",
    "\n",
    "print(f\"Initializing test data for pr_id = '{pr_id}'\")\n",
    "\n",
    "# ---------- 2. Build raw DB name ----------\n",
    "if pr_id == \"prod\":\n",
    "    raw_db_name = \"raw\"\n",
    "else:\n",
    "    raw_db_name = f\"{pr_id}_raw\"\n",
    "\n",
    "print(f\"Using raw DB = {raw_db_name}\")\n",
    "\n",
    "# Make sure the database (schema) exists\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {raw_db_name}\")\n",
    "\n",
    "# ---------- 3. Create a small test dataset (inline) ----------\n",
    "data = [\n",
    "    {\"order_id\": \"o1\", \"customer_id\": \"c1\", \"amount\": 100.0, \"created_at\": \"2024-01-01T10:00:00\"},\n",
    "    {\"order_id\": \"o2\", \"customer_id\": \"c2\", \"amount\": 200.5, \"created_at\": \"2024-02-15T15:30:00\"},\n",
    "    {\"order_id\": \"o3\", \"customer_id\": \"c3\", \"amount\": -50.0, \"created_at\": \"2024-03-10T09:00:00\"},  # will be filtered out\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.StringType(), False),\n",
    "    T.StructField(\"customer_id\", T.StringType(), False),\n",
    "    T.StructField(\"amount\", T.DoubleType(), False),\n",
    "    T.StructField(\"created_at\", T.StringType(), False),  # load as string first\n",
    "])\n",
    "\n",
    "df_raw = spark.createDataFrame(data, schema).withColumn(\n",
    "    \"created_at\",\n",
    "    F.to_timestamp(\"created_at\")\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Test raw data:\")\n",
    "display(df_raw)\n",
    "\n",
    "# ---------- 4. Write to raw table ----------\n",
    "raw_table = f\"{raw_db_name}.orders_raw\"\n",
    "print(f\"Writing test data to raw table: {raw_table}\")\n",
    "\n",
    "(\n",
    "    df_raw\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(raw_table)\n",
    ")\n",
    "\n",
    "print(\"Raw test data initialized âœ…\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "init_test_notebook",
   "widgets": {
    "pr_id": {
     "currentValue": "local_dev",
     "nuid": "53c6cbba-9c48-4531-94cb-55569fc9fa00",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "local_dev",
      "label": null,
      "name": "pr_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "local_dev",
      "label": null,
      "name": "pr_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
